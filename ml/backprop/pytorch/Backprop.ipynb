{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Backprop.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOgG3W0zVtG4LvXSJtfNKw8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/twwhatever/cs101/blob/master/ml/backprop/pytorch/Backprop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXrsZaLlcvll",
        "colab_type": "text"
      },
      "source": [
        "# Backprop example\n",
        "\n",
        "End-to-end illustration of backpropagation using PyTorch.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IioYVkRdDIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PVRiUyHdJcn",
        "colab_type": "text"
      },
      "source": [
        "We will use the x-or dataset as an example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FR3QCyxdGSz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = torch.tensor(\n",
        "    [\n",
        "      [0, 0],\n",
        "      [0, 1],\n",
        "      [1, 0],\n",
        "      [1, 1],\n",
        "    ],\n",
        "    dtype=torch.float32,\n",
        ")\n",
        "y = torch.tensor(\n",
        "    [\n",
        "      0,\n",
        "      1,\n",
        "      1,\n",
        "      0,\n",
        "    ],\n",
        "    dtype=torch.float32,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbi7TBDQdnOF",
        "colab_type": "text"
      },
      "source": [
        "We'll set up a two-layer network.  PyTorch defines a bunch of ready-made layers and architectures, but for this example we'll explicitly build each of the parameters.\n",
        "\n",
        "For exposition purposes, we're setting the weights to a known-good initial point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi52iR9MeUec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Weights for layer 1\n",
        "w1 = torch.tensor([[0.1, 0.3], [0.2, 0.1]], requires_grad=True)\n",
        "# bias for layer 1\n",
        "b1 = torch.tensor([0.2, -0.2], requires_grad=True)\n",
        "# Weights for layer 2\n",
        "w2 = torch.tensor([[0.1], [-0.4]], requires_grad=True)\n",
        "# bias for layer 2\n",
        "b2 = torch.tensor([0.0], requires_grad=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjsV3u95foRQ",
        "colab_type": "text"
      },
      "source": [
        "The first thing we need to do is compute the forward pass through the network and the loss.  We'll use MSE (actually, it's square) as the loss for simplicity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHYURPTdfm8h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "fa78a9c0-09a4-495b-ab9f-dd2495c34f3c"
      },
      "source": [
        "o1 = torch.relu(torch.matmul(x[0], w1) + b1)\n",
        "print(o1)\n",
        "o2 = torch.sigmoid(torch.matmul(o1, w2) + b2)\n",
        "print(o2)\n",
        "# MSE loss\n",
        "loss = (y[0] - o2) ** 2\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.2000, 0.0000], grad_fn=<ReluBackward0>)\n",
            "tensor([0.5050], grad_fn=<SigmoidBackward>)\n",
            "tensor([0.2550], grad_fn=<PowBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lxkQF4vi1GL",
        "colab_type": "text"
      },
      "source": [
        "The forward pass tracks operations on the parameters.  For example, you can see the `.grad_fn` attribute in the tensors above.  \n",
        "\n",
        "Now we use the loss to compute the backward pass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4a-9Aqvh9dT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5e651bcd-758b-4a31-8eb4-beca16f0443e"
      },
      "source": [
        "loss.backward()\n",
        "print(w2.grad, b2.grad)\n",
        "print(w1.grad, b1.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.0505],\n",
            "        [0.0000]]) tensor([0.2525])\n",
            "tensor([[0., 0.],\n",
            "        [0., 0.]]) tensor([0.0252, 0.0000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cef8cO5ZkVhA",
        "colab_type": "text"
      },
      "source": [
        "The backward pass has computed the partial derivatives of the loss with respect to each parameter and stored the results in the `.grad` attribute for each parameter.  \n",
        "\n",
        "Now we just need to perform the gradient step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95Nk_SrijfiA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "6db4bdaf-9a2a-4c92-cbf1-d92b0b7d942d"
      },
      "source": [
        "# learning rate\n",
        "n = 0.01\n",
        "# We don't want to track updates from gradient descent!\n",
        "with torch.no_grad():\n",
        "  w1 -= n * w1.grad\n",
        "  b1 -= n * b1.grad\n",
        "  print(w1, b1)\n",
        "  w2 -= n * w2.grad\n",
        "  b2 -= n * b2.grad\n",
        "  print(w2, b2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.1000, 0.3000],\n",
            "        [0.2000, 0.1000]], requires_grad=True) tensor([ 0.1997, -0.2000], requires_grad=True)\n",
            "tensor([[ 0.0995],\n",
            "        [-0.4000]], requires_grad=True) tensor([-0.0025], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgyteV1Lmncw",
        "colab_type": "text"
      },
      "source": [
        "Now that we've gone through the basics, we'll write some convenience functions that allow us to illustrate a full training run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "279R-lXQkzTm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Forward pass\n",
        "def forward(x):\n",
        "  ol1 = torch.relu(torch.matmul(x, w1) + b1)\n",
        "  ol2 = torch.sigmoid(torch.matmul(ol1, w2) + b2)\n",
        "  return ol2\n",
        "\n",
        "# Gradient update\n",
        "def step(ts):\n",
        "  with torch.no_grad():\n",
        "    for t in ts:\n",
        "      t -= n * t.grad\n",
        "\n",
        "# Zero gradients\n",
        "def zero_grad(ts):\n",
        "  for t in ts:\n",
        "    if t.grad is not None:\n",
        "      t.grad.data.zero_()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNhTtakUn61J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "e408a00c-e63d-46e2-fd4c-948d8e8718a3"
      },
      "source": [
        "EPOCHS = 5000\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  # \"batch size\" of 1\n",
        "  for i in range(4):\n",
        "    zero_grad([w1, b1, w2, b2])\n",
        "    loss = (y[i] - forward(x[i])) ** 2\n",
        "    loss.backward()\n",
        "    step([w1, b1, w2, b2])\n",
        "  # report loss on full dataset every 1000 epochs\n",
        "  if epoch % 1000 == 0:\n",
        "    loss = torch.mean((y - forward(x).squeeze()) ** 2)\n",
        "    print(f\"EPOCH {epoch + 1} complete, loss {loss.item()}\")\n",
        "print(w1, b1)\n",
        "print(w2, b2)\n",
        "for i in range(4):\n",
        "  print(f\"{y[i], forward(x[i])}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH 1 complete, loss 0.24752041697502136\n",
            "EPOCH 1001 complete, loss 0.2040688395500183\n",
            "EPOCH 2001 complete, loss 0.13945071399211884\n",
            "EPOCH 3001 complete, loss 0.05831518396735191\n",
            "EPOCH 4001 complete, loss 0.025512760505080223\n",
            "tensor([[1.6383, 2.2101],\n",
            "        [1.6393, 2.2098]], requires_grad=True) tensor([-5.1987e-04, -2.2097e+00], requires_grad=True)\n",
            "tensor([[ 2.2965],\n",
            "        [-3.8227]], requires_grad=True) tensor([-1.5349], requires_grad=True)\n",
            "(tensor(0.), tensor([0.1773], grad_fn=<SigmoidBackward>))\n",
            "(tensor(1.), tensor([0.9028], grad_fn=<SigmoidBackward>))\n",
            "(tensor(1.), tensor([0.9025], grad_fn=<SigmoidBackward>))\n",
            "(tensor(0.), tensor([0.0789], grad_fn=<SigmoidBackward>))\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}